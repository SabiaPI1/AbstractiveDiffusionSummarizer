# Диффузионная Модель для Абстрактной Суммаризации Текста

Финальный проект для курса "Генеративные задачи в NLP II".
Проект реализует модель абстрактной суммаризации текста с использованием диффузионного подхода, а также включает baseline модель (T5-small) для сравнения. Модель обучается на датасете XSUM.

Цель проекта — разработать и обучить диффузионную модель для задачи абстрактной суммаризации текста. Модель принимает на вход длинный текстовый документ (например, новостную статью) и генерирует краткое, связное саммари, отражающее его основное содержание. Реализация включает кастомную архитектуру Трансформер-энкодер-декодер для диффузионного процесса.

Для оценки качества диффузионной модели предусмотрено обучение и оценка baseline-модели на основе T5-small.

## Возможности

-   **Абстрактная Суммаризация**: генерация саммари, которые могут содержать фразы, отсутствующие в исходном тексте.
-   **Диффузионная Модель**:
    -   Кастомная архитектура на основе Transformer.
    -   Поддержка линейного и косинусного расписания шума.
    -   Реализация прямого (q-sample) и обратного (DDPM, DDIM) диффузионных процессов.
    -   Условная генерация саммари на основе входного документа.
    -   Преобразование эмбеддингов в ID токенов с помощью поиска ближайшего соседа по косинусному сходству.
-   **Baseline Модель**:
    -   T5-small для сравнения производительности.
    -   Обучение и оценка с использованием Hugging Face Trainer.
-   **Оценка**:
    -   Расчет метрик ROUGE (ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-Lsum).
    -   Логирование процесса обучения (loss, val_loss).
-   **Конфигурируемость**: гибкая настройка параметров через `config.yaml`.
-   **Интеграция с Weights & Biases**: для логирования экспериментов.

## Технологический стек

-   Python 3.10+
-   PyTorch
-   Hugging Face Transformers, Datasets, Evaluate
-   NLTK (для токенизации предложений при расчете ROUGE)
-   PyYAML (для конфигурации)
-   NumPy
-   einops
-   tqdm
-   Weights & Biases

## Датасет

-   **XSUM (Extreme Summarization)**: датасет новостных статей BBC и их однофразовых саммари.
-   **Предобработка**: Токенизация текстов документов и саммари с использованием соответствующих токенизаторов для baseline и диффузионной модели.

## Установка и Настройка

1.  **Клонируйте репозиторий:**
    ```bash
    git clone https://github.com/SabiaPI1/AbstractiveDiffusionSummarizer.git
    cd AbstractiveDiffusionSummarizer
    ```

2.  **Установите необходимые библиотеки:**
   
    Убедитесь, что у вас установлены основные Python-библиотеки для машинного обучения и обработки данных. Минимальный список:
    *   `torch`
    *   `transformers`
    *   `datasets`
    *   `evaluate`
    *   `nltk`
    *   `pyyaml`
    *   `numpy`
    *   `einops`
    *   `tqdm`
    *   `wandb` 

3.  **Подготовьте оффлайн компоненты Hugging Face:**
   
    Скрипт ожидает, что предобученные модели и токенизаторы будут находиться в локальной директории `offline_hf_components/`. Скачайте их с Hugging Face Hub и разместите соответствующим образом:
    ```
    offline_hf_components/
    ├── t5-small/
    │   ├── config.json
    │   ├── pytorch_model.bin
    │   └── ... (другие файлы модели)
    └── distilbert-base-uncased/
        ├── config.json
        ├── pytorch_model.bin
        └── ... (другие файлы токенизатора)
    ```
    Это актуально для работы на системах с ограниченным доступом в интернет.

4.  **Настройка Weights & Biases:**
    Если вы планируете использовать W&B:
    -   Установите библиотеку: `pip install wandb`
    -   Авторизуйтесь: `wandb login` (или установите переменную окружения `WANDB_API_KEY`).
    -   Включите логирование в `config.yaml` (`enable_wandb: True`).

## Конфигурация

Все параметры обучения, моделей и датасета настраиваются в файле `config.yaml`. Ключевые параметры включают:

-   `enable_wandb`, `wandb_project`, `wandb_run_name`: настройки W&B.
-   `dataset_name`, `train_size`, `val_size`, `test_size`: параметры датасета.
-   `seed`, `device`: общие настройки.
-   `run_baseline_first`: запускать ли обучение baseline перед диффузионной моделью.
-   `baseline`: секция с параметрами для T5 модели (имя модели, длины, batch_size, lr и т.д.).
-   `diffusion`: секция с параметрами для диффузионной модели (имя токенизатора, параметры диффузии, архитектуры модели, обучения, генерации).

## Запуск проекта
Основной скрипт для запуска всех этапов проекта — `summarization_project.py`.

```bash
python summarization_project.py --config config.yaml
```


*Основной фокус текущей работы — отладка диффузионной модели для достижения приемлемого качества генерации осмысленного и релевантного текста.*

## Будущие доработки

После достижения удовлетворительного качества генерации, планируется:

-   Реализация **Latent Diffusion**: Перенос диффузионного процесса в латентное пространство.
-   Использование более **продвинутых архитектур** или предобученных эмбеддингов (например, от более крупных моделей) для инициализации.
-   Исследование **Classifier Guidance** для управления стилем или тематикой саммари.
